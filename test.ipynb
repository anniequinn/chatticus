{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The article \"Explicit perception versus emotion in multi-crises\" by Marcel Buettner and Lukas Burs of GroupM Science delves into the impact of ongoing global crises on consumer behavior and emotions. The authors challenge the simplistic notion that crises solely lead to negative consumer sentiments and reduced spending. Instead, they suggest that multi-crises can result in the formation of \\'crisis-proof\\' habits that act as a conscious defense mechanism, enabling consumers to maintain stability and orientation in their lives.\\n\\nMoreover, the article discusses how consumers perceive global crises differently from their emotional responses to these crises. It emphasizes the need to understand the nuanced relationship between consumer perceptions, emotions, and behaviors during times of crisis. By recognizing and adapting to these complexities, brands can effectively tailor their advertising strategies to resonate with consumers amidst multi-crises.\\n\\nIn the context of advertising accessibility for visually impaired audiences, the article highlights the importance of making advertising more inclusive through audio enhancements. By incorporating audio elements, such as descriptions of visual content, brands can ensure that visually impaired individuals can also engage with and benefit from advertising materials.\\n\\nIf you have any specific questions or require further details on the content of this article or any other article from Atticus Journal, Volume 28, feel free to ask!'"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "from scipy import spatial\n",
    "from langchain_openai import ChatOpenAI, OpenAIEmbeddings\n",
    "from langchain.schema import SystemMessage, HumanMessage\n",
    "\n",
    "def compute_similarity(x, y):\n",
    "    return 1 - spatial.distance.cosine(x, y)\n",
    "\n",
    "load_dotenv()\n",
    "api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "\n",
    "chat_model_name = \"gpt-3.5-turbo\"\n",
    "embed_model_name = \"text-embedding-ada-002\"\n",
    "embeddings = OpenAIEmbeddings(model=embed_model_name, api_key=api_key)\n",
    "chat = ChatOpenAI(model_name=chat_model_name)\n",
    "\n",
    "db = pd.read_parquet(\"data/vector_db.parquet\")\n",
    "n_queries = 5\n",
    "\n",
    "context = \"\"\"\n",
    "    You are a virtual assistant specifically designed to provide information about WPP's Atticus Journal, Volume 28. \n",
    "    You have access to a knowledge base of the specific articles within this edition of the journal. \n",
    "    Your purpose is to assist users with inquiries directly related to this journal, including discussions on marketing strategies, creative insights, and case studies presented within this volume.\n",
    "    When users ask questions, your responses should be accurate and relevant to the information contained in Atticus Journal, Volume 28. If a user asks a question that falls outside the scope of this journal or relates to general topics not covered within it, you should respond with:\n",
    "    'Sorry, I can't help with that. I can only answer questions relating to the Atticus Journal, Volume 28. Please feel free to ask about the content, themes, or specific articles within this edition.'\n",
    "    Remember, your goal is to be helpful and informative about the Atticus Journal, Volume 28, while clearly communicating the boundaries of your expertise.\n",
    "\"\"\"\n",
    "\n",
    "messages = [SystemMessage(content=context)]\n",
    "\n",
    "user_query = \"Tell me about multicrises and advertising\"\n",
    "embedded_query = embeddings.embed_query(user_query)\n",
    "\n",
    "# Use cosine similarity to get the n most similar texts\n",
    "similarities_array = np.array([\n",
    "    compute_similarity(embedded_query, embedding) for embedding in db[\"embedding\"].values\n",
    "]) \n",
    "top_indices = np.argsort(similarities_array)[::-1][:n_queries]\n",
    "matches = db.iloc[top_indices]\n",
    "\n",
    "appended_user_query = user_query\n",
    "appended_user_query += \"Based on the information retrieved, consider the following text to guide your response:\\n\\n\"\n",
    "appended_user_query += \"\\n\\n\".join(matches[\"text\"].values)\n",
    "\n",
    "messages.append(HumanMessage(content=appended_user_query))\n",
    "\n",
    "response = chat(messages)\n",
    "response.content"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "persona-pathways",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
